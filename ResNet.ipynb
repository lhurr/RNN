{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Resizing, Rescaling, Reshape\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer, Normalization, Flatten,BatchNormalization, Dropout\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "from tensorflow.keras.optimizers.schedules import *\n",
    "from tensorflow.keras.metrics import FalseNegatives, binary_crossentropy, binary_accuracy, categorical_crossentropy, sparse_categorical_crossentropy\n",
    "\n",
    "from IPython.display import Audio, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(keras.utils.Sequence):\n",
    "#     'Generates data for Keras'\n",
    "#     def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "#                  n_classes=10, shuffle=True):\n",
    "#         'Initialization'\n",
    "#         self.dim = dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.labels = labels\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         'Denotes the number of batches per epoch'\n",
    "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         'Generate one batch of data'\n",
    "#         # Generate indexes of the batch\n",
    "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#         # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "#         # Generate data\n",
    "#         X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         'Updates indexes after each epoch'\n",
    "#         self.indexes = np.arange(len(self.list_IDs))\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "#         # Generate data\n",
    "#         for i, ID in enumerate(list_IDs_temp):\n",
    "#             # Store sample\n",
    "#             X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "#             # Store class\n",
    "#             y[i] = self.labels[ID]\n",
    "\n",
    "#         return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "# a = to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "# a = tf.constant(4, shape=[4, 4])\n",
    "# print(a)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chanDim, red=False,\n",
    "        reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        # the shortcut branch of the ResNet module should be\n",
    "        # initialize as the input (identity) data\n",
    "        shortcut = data\n",
    "\n",
    "        # the first block of the ResNet module are the 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(data)\n",
    "        act1 = Activation(\"relu\")(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # the second block of the ResNet module are the 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(conv1)\n",
    "        act2 = Activation(\"relu\")(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "            padding=\"same\", use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "        # the third block of the ResNet module is another set of 1x1\n",
    "        # CONVs\n",
    "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(conv2)\n",
    "        act3 = Activation(\"relu\")(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "        # if we are to reduce the spatial size, apply a CONV layer to\n",
    "        # the shortcut\n",
    "        if red:\n",
    "            shortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "                use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # add together the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "\n",
    "        # return the addition as the output of the ResNet module\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, stages, filters,\n",
    "        reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(inputs)\n",
    "\n",
    "        # apply a single CONV layer\n",
    "        x = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "            padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "        # loop over the number of stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # initialize the stride, then apply a residual module\n",
    "            # used to reduce the spatial size of the input volume\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "            x = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "                chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "            # loop over the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a ResNet module\n",
    "                x = ResNet.residual_module(x, filters[i + 1],\n",
    "                    (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "        # apply BN => ACT => POOL\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    def __init__(self, inputs) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.normalize = Normalization()\n",
    "        self.optimizer = Adam(learning_rate=0.5)\n",
    "        self.dense1 = Dense(1028, activation = 'relu')\n",
    "        self.dense2 = Dense(128, activation='relu')\n",
    "        self.dense3 = Dense(128, activation='relu')\n",
    "        self.dense4 = Dense(10, activation ='softmax')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.model = Model\n",
    "    def call(self, training = False):\n",
    "        self.normalize.adapt(X_train)\n",
    "        x= self.normalize(self.inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.model(inputs = self.inputs, outputs = x)\n",
    "    def print(self):\n",
    "        model = self.call()\n",
    "        print(model.summary())\n",
    "    def compile(self):\n",
    "        model = self.call()\n",
    "        model.compile(optimizer = self.optimizer, loss = 'mean_squared_error', metrics='accuracy')\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eec5519e01a69f2e1873432b3cb768f5f3c8fa317c979519b1fcf80dc68a57ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
