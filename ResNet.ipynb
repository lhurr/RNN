{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "# a = to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "# a = tf.constant(4, shape=[4, 4])\n",
    "# print(a)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chanDim, red=False,\n",
    "        reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        # the shortcut branch of the ResNet module should be\n",
    "        # initialize as the input (identity) data\n",
    "        shortcut = data\n",
    "\n",
    "        # the first block of the ResNet module are the 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(data)\n",
    "        act1 = Activation(\"relu\")(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # the second block of the ResNet module are the 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(conv1)\n",
    "        act2 = Activation(\"relu\")(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "            padding=\"same\", use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "        # the third block of the ResNet module is another set of 1x1\n",
    "        # CONVs\n",
    "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(conv2)\n",
    "        act3 = Activation(\"relu\")(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "            kernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "        # if we are to reduce the spatial size, apply a CONV layer to\n",
    "        # the shortcut\n",
    "        if red:\n",
    "            shortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "                use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # add together the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "\n",
    "        # return the addition as the output of the ResNet module\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, stages, filters,\n",
    "        reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(inputs)\n",
    "\n",
    "        # apply a single CONV layer\n",
    "        x = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "            padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "        # loop over the number of stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # initialize the stride, then apply a residual module\n",
    "            # used to reduce the spatial size of the input volume\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "            x = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "                chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "            # loop over the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a ResNet module\n",
    "                x = ResNet.residual_module(x, filters[i + 1],\n",
    "                    (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "        # apply BN => ACT => POOL\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "            momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    def __init__(self, inputs) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.normalize = Normalization()\n",
    "        self.optimizer = Adam(learning_rate=0.5)\n",
    "        self.dense1 = Dense(1028, activation = 'relu')\n",
    "        self.dense2 = Dense(128, activation='relu')\n",
    "        self.dense3 = Dense(128, activation='relu')\n",
    "        self.dense4 = Dense(10, activation ='softmax')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.model = Model\n",
    "    def call(self, training = False):\n",
    "        self.normalize.adapt(X_train)\n",
    "        x= self.normalize(self.inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.model(inputs = self.inputs, outputs = x)\n",
    "    def print(self):\n",
    "        model = self.call()\n",
    "        print(model.summary())\n",
    "    def compile(self):\n",
    "        model = self.call()\n",
    "        model.compile(optimizer = self.optimizer, loss = 'mean_squared_error', metrics='accuracy')\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eec5519e01a69f2e1873432b3cb768f5f3c8fa317c979519b1fcf80dc68a57ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
